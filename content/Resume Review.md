---
title: Review
tags: 
date: 2024-02-16
aliases: 
draft: "true"
---
## Polymath

## VIP
- FLAIROP project – collaboration with a bunch of German & Canadian institutions and companies 

## NRC
- COVID-Net project – initiative from UWaterloo and NRC for ML-assisted COVID measures
- I worked on COVID diagnosis from lung radiographs
- The core problem I worked on was addressing the data weaknesses that come with novel diseases:
	- Lack of data
	- Imbalanced data – a lot more benign (negative) than malignant (positive)
- Small datasets means that transfer learning is good, but the success of transfer learning is heavily dependent on the relevance of upstream data, and the it requires high quality labels, which is costly for medical imaging
- To combat this, we used supervised pre-training on ImageNet, self-supervised pre-training on chest X-ray dataset (not COVID), then finally supervised fine-tuning on COVID datasets.
	- SimCLR – contrastive method from Google; do data augmentation, use a encoder to extract representation vectors from images, and then train use a contrastive loss function on the vectors – basically just want to determine if two augmentation are from the same original image, or from different origins.
	- MoCo (momentum contrast) – contrastive method from Facebook that uses a dictionary look-up mechanism. 
		- For each image, generate 2 augmentations 
		- One is fed into a query encoder network, which produces a feature vector
		- Other is fed into the key encoder, which produces the key vector.
		- Key encoder and query encoder have the same architecture but different params.
		- They key encoders parameters are updated as a moving average of the query encoder's parameters, but with a lag introduced by a momentum term. This slow update helps in maintaining consistency in the representations learned over time.
		- MoCo maintains a dynamically updated dictionary (queue) of keys. This dictionary stores the representations (keys) generated by the key encoder from previous batches.
		- For a given query, its positive key is the one generated from the same image but through the key encoder. Negative keys are all other keys in the dictionary.
		- Use a contrastive loss function to pull the query closer to its positive key while pushing it away from the negative keys. The loss is minimized when the query is similar to its positive key and dissimilar from the negative keys.
		- After processing a batch, the keys generated by the key encoder are added to the dictionary. If the dictionary exceeds its size limit, the oldest keys are removed to make space for the new ones
		- Weakness: Having two separate encoders (query and key encoders) increases the complexity of the model architecture, sensitive to the choice of the momentum coefficient used to update the key encoder, size and management of the dynamic dictionary of keys can significantly impact the model's performance.
- For data imbalance in the fine-tuning process, we used an AUC maximization loss function.
	- ROC (Receiver Operating Characteristic) Curve – plot true positive rate against false positive rate for various threshold settings. 
	- A perfectly performing classifier would have AUC = 1, since it would have no false positives at any threshold.
	- AUC is generally a good metric for imbalanced distributions, as it evaluates the model's ability to discriminate between classes rather than its overall correctness. Thus, it makes sense to directly optimize for it during the training process by using it for loss.
- Trustworthiness and explainability – with this many data issues and considering the importance of medical imaging, it's important to verify and quantify these
	- Explainability – attention maps verified by radiologists
	- Trustworthiness – trust score formulated based on confidence + correctness
- Didn't make too many fundamental contributions – more of an engineering task of putting the right pieces together to solve the problem at hand
	- Doing experiments, ablation studies, visualizations, to verify the efficacy of each component

## WATO

## Projects