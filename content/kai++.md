---

kanban-plugin: basic
draft: "true"

---

## Today

- [ ] makemore (part 1)
- [ ] makemore (part 2)
- [ ] makemore (part 3)
- [ ] makemore (part 4)
- [ ] makemore (part 5)
- [ ] gpt from scratch


## UDL

- [ ] 3 - Shallow neural networks
- [ ] 3.1 - Neural network example
- [ ] 3.2 - Universal approximation theorem
- [ ] 3.3 - Multivariate inputs and outputs
- [ ] 3.4 - Shallow neural networks: general case
- [ ] 3.5 - Terminology
- [ ] 3.6 - Summary
- [ ] 4 - Deep neural networks
- [ ] 4.1 - Composing neural networks
- [ ] 4.2 - From composing networks to deep networks
- [ ] 4.3 - Deep neural networks
- [ ] 4.4 - Matrix notation
- [ ] 4.5 - Shallow vs. deep neural networks
- [ ] 4.6 - Summary
- [ ] 5 - Loss functions
- [ ] 5.1 - Maximum likelihood
- [ ] 5.2 - Recipe for constructing loss functions
- [ ] 5.3 - Example 1: univariate regression
- [ ] 5.4 - Example 2: binary classification
- [ ] 5.5 - Example 3: multiclass classification
- [ ] 5.6 - Multiple outputs
- [ ] 5.7 - Cross-entropy loss
- [ ] 5.8 - Summary
- [ ] 6 - Fitting models
- [ ] 6.1 - Gradient descent
- [ ] 6.2 - Stochastic gradient descent
- [ ] 6.3 - Momentum
- [ ] 6.4 - Adam
- [ ] 6.5 - Training algorithm hyperparameters
- [ ] 6.6 - Summary
- [ ] 7 - Gradients and initialization
- [ ] 7.1 - Problem definitions
- [ ] 7.2 - Computing derivatives
- [ ] 7.3 - Toy example
- [ ] 7.4 - Backpropagation algorithm
- [ ] 7.5 - Parameter initialization
- [ ] 7.6 - Example training code
- [ ] 7.7 Summary
- [ ] 8 - Measuring performance
- [ ] 8.1 - Training a simple model
- [ ] 8.2 - Sources of error
- [ ] 8.3 - Reducing error
- [ ] 8.4 - Double descent
- [ ] 8.5 - Choosing hyperparameters
- [ ] 8.6 - Summary
- [ ] 9 - Regularization
- [ ] 9.1 Explicit regularization
- [ ] 9.2 Implicit regularization
- [ ] 9.3 Heuristics to improve performance
- [ ] 9.4 Summary
- [ ] 10 - Convolutional networks
- [ ] 10.1 - Invariance and equivariance
- [ ] 10.2 - Convolutional networks for 1D inputs
- [ ] 10.3 - Convolutional networks for 2D inputs
- [ ] 10.4 - Downsampling and upsampling
- [ ] 10.5 - Applications
- [ ] 10.6 - Summary
- [ ] 11 - Residual networks
- [ ] 11.1 - Sequential processing
- [ ] 11.2 - Residual connections and residual blocks
- [ ] 11.3 - Exploding gradients in residual networks
- [ ] 11.4 - Batch normalization
- [ ] 11.5 - Common residual architectures
- [ ] 11.6 - Why do nets with residual connections perform so well?
- [ ] 11.7 - Summary
- [ ] 12 - Transformers
- [ ] 12.1 - Processing text data
- [ ] 12.2 - Dot-product self-attention
- [ ] 12.3 - Extensions to dot-product self-attention
- [ ] 12.4 - Transformers
- [ ] 12.5 - Transformers for natural language processing
- [ ] 12.6 - Encoder model example: BERT
- [ ] 12.7 - Decoder model example: GPT3
- [ ] 12.8 - Encoder-decoder model example: machine translation
- [ ] 12.9 - Transformers for long sequences
- [ ] 12.10 -  Transformers for images
- [ ] 12.11 - Summary
- [ ] 13 - Graph neural networks
- [ ] 13.1 -  What is a graph?
- [ ] 13.2 - Graph representation
- [ ] 13.3 - Graph neural networks, tasks, and loss functions
- [ ] 13.4 - Graph convolutional networks
- [ ] 13.5 - Example: graph classification
- [ ] 13.6 - Inductive vs. transductive models
- [ ] 13.7 - Example: node classification
- [ ] 13.8 - Layers for graph convolutional networks
- [ ] 13.9 - Edge graphs
- [ ] 13.10 - Summary
- [ ] 14 - Unsupervised learning
- [ ] 14.1 - Taxonomy of unsupervised learning models
- [ ] 14.2 - What makes a good generative model?
- [ ] 14.3 - Quantifying performance
- [ ] 14.4 - Summary
- [ ] 15 - Generative Adversarial Networks
- [ ] 15.1 - Discrimination as a signal
- [ ] 15.2 - Improving stability
- [ ] 15.3 - Progressive growing, mini-batch discrimination, and truncation
- [ ] 15.4 - Conditional generation
- [ ] 15.5 - Image translation
- [ ] 15.6 - StyleGAN
- [ ] 15.7 - Summary
- [ ] 16 - Normalizing flows
- [ ] 16.1 - 1D example
- [ ] 16.2 - General case
- [ ] 16.3 - Invertible network layers
- [ ] 16.4 - Multi-scale flows
- [ ] 16.5 - Applications
- [ ] 16.6 - Summary
- [ ] 17 - Variational autoencoders
- [ ] 17.1 - Latent variable models
- [ ] 17.2 - Nonlinear latent variable model
- [ ] 17.3 - Training
- [ ] 17.4 - ELBO properties
- [ ] 17.5 - Variational approximation
- [ ] 17.6 - The variational autoencoder
- [ ] 17.7 - The reparameterization trick
- [ ] 17.8 - Applications
- [ ] 17.9 - Summary
- [ ] 18 - Diffusion models
- [ ] 18.1 - Overview
- [ ] 18.2 - Encoder (forward process)
- [ ] 18.3 - Decoder model (reverse process)
- [ ] 18.4 - Training
- [ ] 18.5 - Re-parameterization of loss function
- [ ] 18.6 - Implementation
- [ ] 18.7 - Summary
- [ ] 19 - Reinforcement learning
- [ ] 19.1 - Markov decision processes, returns, and policies
- [ ] 19.2 - Expected return
- [ ] 19.3 - Tabular reinforcement learning
- [ ] 19.4 - Fitted Q-learning
- [ ] 19.5 - Policy gradient methods
- [ ] 19.6 - Actor-critic methods
- [ ] 19.7 - Offline reinforcement learning
- [ ] 19.8 - Summary
- [ ] 20 - Why does deep learning work?
- [ ] 20.1 - The case against deep learning
- [ ] 20.2 - Factors that influence fitting performance
- [ ] 20.3 - Properties of loss functions
- [ ] 20.4 - Factors that determine generalization
- [ ] 20.5 - Do we need so many parameters?
- [ ] 20.6 - Do networks have to be deep?
- [ ] 20.7 - Summary


## DLFC

- [ ] 1 - The Deep Learning Revolution
- [ ] 1.2 - A Tutorial Example
- [ ] 1.2.1 - Synthetic data
- [ ] 1.2.2 - Linear models
- [ ] 1.2.3 - Error function
- [ ] 1.2.4 - Model complexity
- [ ] 1.2.5 - Regularization
- [ ] 1.2.6 - Model selection
- [ ] 1.3 - A Brief History of Machine Learning
- [ ] 1.3.1 - Single-layer networks
- [ ] 1.3.2 - Backpropagation
- [ ] 1.3.3 - Deep networks
- [ ] 2 - Probabilities
- [ ] 2.1 - The Rules of Probability
- [ ] 2.1.1 - A medical screening example
- [ ] 2.1.2 - The sum and product rules
- [ ] 2.1.3 - Bayes’ theorem
- [ ] 2.1.4 - Medical screening revisited
- [ ] 2.1.5 - Prior and posterior probabilities
- [ ] 2.1.6 - Independent variables
- [ ] 2.2 - Probability Densities
- [ ] 2.2.1 - Example distributions
- [ ] 2.2.2 - Expectations and covariances
- [ ] 2.3 - The Gaussian Distribution
- [ ] 2.3.1 - Mean and variance
- [ ] 2.3.2 - Likelihood function
- [ ] 2.3.3 - Bias of maximum likelihood
- [ ] 2.3.4 - Linear regression
- [ ] 2.4 - Transformation of Densities
- [ ] 2.4.1 - Multivariate distributions
- [ ] 2.5 - Information Theory
- [ ] 2.5.1 - Entropy
- [ ] 2.5.2 - Physics perspective
- [ ] 2.5.3 - Differential entropy
- [ ] 2.5.4 - Maximum entropy
- [ ] 2.5.5 - Kullback–Leibler divergence
- [ ] 2.5.6 - Conditional entropy
- [ ] 2.5.7 - Mutual information
- [ ] 2.6 - Bayesian Probabilities
- [ ] 2.6.1 - Model parameters
- [ ] 2.6.2 - Regularization
- [ ] 2.6.3 - Bayesian machine learning
- [ ] 3 - Standard Distributions
- [ ] 3.1 - Discrete Variables
- [ ] 3.1.1 - Bernoulli distribution
- [ ] 3.1.2 - Binomial distribution
- [ ] 3.1.3 - Multinomial distribution
- [ ] 3.2 - The Multivariate Gaussian
- [ ] 3.2.1 - Geometry of the Gaussian
- [ ] 3.2.2 - Moments
- [ ] 3.2.3 - Limitations
- [ ] 3.2.4 - Conditional distribution
- [ ] 3.2.5 - Marginal distribution
- [ ] 3.2.6 - Bayes’ theorem
- [ ] 3.2.7 - Maximum likelihood
- [ ] 3.2.8 - Sequential estimation
- [ ] 3.2.9 - Mixtures of Gaussians
- [ ] 3.3 - Periodic Variables
- [ ] 3.3.1 - Von Mises distribution
- [ ] 3.4 - The Exponential Family
- [ ] 3.4.1 - Sufficient statistics
- [ ] 3.5 - Nonparametric Methods
- [ ] 3.5.1 - Histograms
- [ ] 3.5.2 - Kernel densities
- [ ] 3.5.3 - Nearest-neighbours
- [ ] 4 - Single-layer Networks: Regression
- [ ] 4.1 - Linear Regression
- [ ] 4.1.1 - Basis functions
- [ ] 4.1.2 - Likelihood function
- [ ] 4.1.3 - Maximum likelihood
- [ ] 4.1.4 - Geometry of least squares
- [ ] 4.1.5 - Sequential learning
- [ ] 4.1.6 - Regularized least squares
- [ ] 4.1.7 - Multiple outputs
- [ ] 4.2 - Decision theory
- [ ] 4.3 - The Bias–Variance Trade-off
- [ ] 5 - Single-layer Networks: Classification
- [ ] 5.1 - Discriminant Functions
- [ ] 5.1.1 - Two classes
- [ ] 5.1.2 - Multiple classes
- [ ] 5.1.3 - 1-of-K coding
- [ ] 5.1.4 - Least squares for classification
- [ ] 5.2 - Decision Theory
- [ ] 5.2.1 - Misclassification rate
- [ ] 5.2.2 - Expected loss
- [ ] 5.2.3 - The reject option
- [ ] 5.2.4 - Inference and decision
- [ ] 5.2.5 - Classifier accuracy
- [ ] 5.2.6 - ROC curve
- [ ] 5.3 - Generative Classifiers
- [ ] 5.3.1 - Continuous inputs
- [ ] 5.3.2 - Maximum likelihood solution
- [ ] 5.3.3 - Discrete features
- [ ] 5.3.4 - Exponential family
- [ ] 5.4 - Discriminative Classifiers
- [ ] 5.4.1 - Activation functions
- [ ] 5.4.2 - Fixed basis functions
- [ ] 5.4.3 - Logistic regression
- [ ] 5.4.4 - Multi-class logistic regression
- [ ] 5.4.5 - Probit regression
- [ ] 5.4.6 - Canonical link functions
- [ ] 6 - Deep Neural Networks
- [ ] 6.1 - Limitations of Fixed Basis Functions
- [ ] 6.1.1 - The curse of dimensionality
- [ ] 6.1.2 - High-dimensional spaces
- [ ] 6.1.3 - Data manifolds
- [ ] 6.1.4 - Data-dependent basis functions
- [ ] 6.2 - Multilayer Networks
- [ ] 6.2.1 - Parameter matrices
- [ ] 6.2.2 - Universal approximation
- [ ] 6.2.3 - Hidden unit activation functions
- [ ] 6.2.4 - Weight-space symmetries
- [ ] 6.3 - Deep Networks
- [ ] 6.3.1 - Hierarchical representations
- [ ] 6.3.2 - Distributed representations
- [ ] 6.3.3 - Representation learning
- [ ] 6.3.4 - Transfer learning
- [ ] 6.3.5 - Contrastive learning
- [ ] 6.3.6 - General network architectures
- [ ] 6.3.7 - Tensors
- [ ] 6.4 - Error Functions
- [ ] 6.4.1 - Regression
- [ ] 6.4.2 - Binary classification
- [ ] 6.4.3 - Multiclass classification
- [ ] 6.5 - Mixture Density Networks
- [ ] 6.5.1 - Robot kinematics example
- [ ] 6.5.2 - Conditional mixture distributions
- [ ] 6.5.3 - Gradient optimization
- [ ] 6.5.4 - Predictive distribution
- [ ] 7 - Gradient Descent
- [ ] 7.1 - Error Surfaces
- [ ] 7.1.1 - Local quadratic approximation
- [ ] 7.2 - Gradient Descent Optimization
- [ ] 7.2.1 - Use of gradient information
- [ ] 7.2.2 - Batch gradient descent
- [ ] 7.2.3 - Stochastic gradient descent
- [ ] 7.2.4 - Mini-batches
- [ ] 7.2.5 - Parameter initialization
- [ ] 7.3 - Convergence
- [ ] 7.3.1 - Momentum
- [ ] 7.3.2 - Learning rate schedule
- [ ] 7.3.3 - RMSProp and Adam
- [ ] 7.4 - Normalization
- [ ] 7.4.1 - Data normalization
- [ ] 7.4.2 - Batch normalization
- [ ] 7.4.3 - Layer normalization
- [ ] 8 - Backpropagation
- [ ] 8.1 - Evaluation of Gradients
- [ ] 8.1.1 - Single-layer networks
- [ ] 8.1.2 - General feed-forward networks
- [ ] 8.1.3 - A simple example
- [ ] 8.1.4 - Numerical differentiation
- [ ] 8.1.5 - The Jacobian matrix
- [ ] 8.1.6 - The Hessian matrix
- [ ] 8.2 - Automatic Differentiation
- [ ] 8.2.1 - Forward-mode automatic differentiation
- [ ] 8.2.2 - Reverse-mode automatic differentiation
- [ ] 9 - Regularization
- [ ] 9.1 - Inductive Bias
- [ ] 9.1.1 - Inverse problems
- [ ] 9.1.2 - No free lunch theorem
- [ ] 9.1.3 - Symmetry and invariance
- [ ] 9.1.4 - Equivariance
- [ ] 9.2 - Weight Decay
- [ ] 9.2.1 - Consistent regularizers
- [ ] 9.2.2 - Generalized weight decay
- [ ] 9.3 - Learning Curves
- [ ] 9.3.1 - Early stopping
- [ ] 9.3.2 - Double descent
- [ ] 9.4 - Parameter Sharing
- [ ] 9.4.1 - Soft weight sharing
- [ ] 9.5 - Residual Connections
- [ ] 9.6 - Model Averaging
- [ ] 9.6.1 - Dropout
- [ ] 10 - Convolutional Networks
- [ ] 10.1 - Computer Vision
- [ ] 10.1.1 - Image data
- [ ] 10.2 - Convolutional Filters
- [ ] 10.2.1 - Feature detectors
- [ ] 10.2.2 - Translation equivariance
- [ ] 10.2.3 - Padding
- [ ] 10.2.4 - Strided convolutions
- [ ] 10.2.5 - Multi-dimensional convolutions
- [ ] 10.2.6 - Pooling
- [ ] 10.2.7 - Multilayer convolutions
- [ ] 10.2.8 - Example network architectures
- [ ] 10.3 - Visualizing Trained CNNs
- [ ] 10.3.1 - Visual cortex
- [ ] 10.3.2 - Visualizing trained filters
- [ ] 10.3.3 - Saliency maps
- [ ] 10.3.4 - Adversarial attacks
- [ ] 10.3.5 - Synthetic images
- [ ] 10.4 - Object Detection
- [ ] 10.4.1 - Bounding boxes
- [ ] 10.4.2 - Intersection-over-union
- [ ] 10.4.3 - Sliding windows
- [ ] 10.4.4 - Detection across scales
- [ ] 10.4.5 - Non-max suppression
- [ ] 10.4.6 - Fast region CNNs
- [ ] 10.5 - Image Segmentation
- [ ] 10.5.1 - Convolutional segmentation
- [ ] 10.5.2 - Up-sampling
- [ ] 10.5.3 - Fully convolutional networks
- [ ] 10.5.4 - The U-net architecture
- [ ] 10.6 - Style Transfer
- [ ] 11 - Structured Distributions
- [ ] 11.1 - Graphical Models
- [ ] 11.1.1 - Directed graphs
- [ ] 11.1.2 - Factorization
- [ ] 11.1.3 - Discrete variables
- [ ] 11.1.4 - Gaussian variables
- [ ] 11.1.5 - Binary classifier
- [ ] 11.1.6 - Parameters and observations
- [ ] 11.1.7 - Bayes’ theorem
- [ ] 11.2 - Conditional Independence
- [ ] 11.2.1 - Three example graphs
- [ ] 11.2.2 - Explaining away
- [ ] 11.2.3 - D-separation
- [ ] 11.2.4 - Naive Bayes
- [ ] 11.2.5 - Generative models
- [ ] 11.2.6 - Markov blanket
- [ ] 11.2.7 - Graphs as filters
- [ ] 11.3 - Sequence Models
- [ ] 11.3.1 - Hidden variables
- [ ] 12 - Transformers
- [ ] 12.1 - Attention
- [ ] 12.1.1 - Transformer processing
- [ ] 12.1.2 - Attention coefficients
- [ ] 12.1.3 - Self-attention
- [ ] 12.1.4 - Network parameters
- [ ] 12.1.5 - Scaled self-attention
- [ ] 12.1.6 - Multi-head attention
- [ ] 12.1.7 - Transformer layers
- [ ] 12.1.8 - Computational complexity
- [ ] 12.1.9 - Positional encoding
- [ ] 12.2 - Natural Language
- [ ] 12.2.1 - Word embedding
- [ ] 12.2.2 - Tokenization
- [ ] 12.2.3 - Bag of words
- [ ] 12.2.4 - Autoregressive models
- [ ] 12.2.5 - Recurrent neural networks
- [ ] 12.2.6 - Backpropagation through time
- [ ] 12.3 - Transformer Language Models
- [ ] 12.3.1 - Decoder transformers
- [ ] 12.3.2 - Sampling strategies
- [ ] 12.3.3 - Encoder transformers
- [ ] 12.3.4 - Sequence-to-sequence transformers
- [ ] 12.3.5 - Large language models
- [ ] 12.4 - Multimodal Transformers
- [ ] 12.4.1 - Vision transformers
- [ ] 12.4.2 - Generative image transformers
- [ ] 12.4.3 - Audio data
- [ ] 12.4.4 - Text-to-speech
- [ ] 12.4.5 - Vision and language transformers
- [ ] 13 - Graph Neural Networks
- [ ] 13.1 - Machine Learning on Graphs
- [ ] 13.1.1 - Graph properties
- [ ] 13.1.2 - Adjacency matrix
- [ ] 13.1.3 - Permutation equivariance
- [ ] 13.2 - Neural Message-Passing
- [ ] 13.2.1 - Convolutional filters
- [ ] 13.2.2 - Graph convolutional networks
- [ ] 13.2.3 - Aggregation operators
- [ ] 13.2.4 - Update operators
- [ ] 13.2.5 - Node classification
- [ ] 13.2.6 - Edge classification
- [ ] 13.2.7 - Graph classification
- [ ] 13.3 - General Graph Networks
- [ ] 13.3.1 - Graph attention networks
- [ ] 13.3.2 - Edge embeddings
- [ ] 13.3.3 - Graph embeddings
- [ ] 13.3.4 - Over-smoothing
- [ ] 13.3.5 - Regularization
- [ ] 13.3.6 - Geometric deep learning
- [ ] 14 - Sampling
- [ ] 14.1 - Basic Sampling Algorithms
- [ ] 14.1.1 - Expectations
- [ ] 14.1.2 - Standard distributions
- [ ] 14.1.3 - Rejection sampling
- [ ] 14.1.4 - Adaptive rejection sampling
- [ ] 14.1.5 - Importance sampling
- [ ] 14.1.6 - Sampling-importance-resampling
- [ ] 14.2 - Markov Chain Monte Carlo
- [ ] 14.2.1 - The Metropolis algorithm
- [ ] 14.2.2 - Markov chains
- [ ] 14.2.3 - The Metropolis–Hastings algorithm
- [ ] 14.2.4 - Gibbs sampling
- [ ] 14.2.5 - Ancestral sampling
- [ ] 14.3 - Langevin Sampling
- [ ] 14.3.1 - Energy-based models
- [ ] 14.3.2 - Maximizing the likelihood
- [ ] 14.3.3 - Langevin dynamics
- [ ] 15 - Discrete Latent Variables
- [ ] 15.1 - K-means Clustering
- [ ] 15.1.1 - Image segmentation
- [ ] 15.2 - Mixtures of Gaussians
- [ ] 15.2.1 - Likelihood function
- [ ] 15.2.2 - Maximum likelihood
- [ ] 15.3 - Expectation–Maximization Algorithm
- [ ] 15.3.1 - Gaussian mixtures
- [ ] 15.3.2 - Relation to K-means
- [ ] 15.3.3 - Mixtures of Bernoulli distributions
- [ ] 15.4 - Evidence Lower Bound
- [ ] 15.4.1 - EM revisited
- [ ] 15.4.2 - Independent and identically distributed data
- [ ] 15.4.3 - Parameter priors
- [ ] 15.4.4 - Generalized EM
- [ ] 15.4.5 - Sequential EM
- [ ] 16 - Continuous Latent Variables
- [ ] 16.1 - Principal Component Analysis
- [ ] 16.1.1 - Maximum variance formulation
- [ ] 16.1.2 - Minimum-error formulation
- [ ] 16.1.3 - Data compression
- [ ] 16.1.4 - Data whitening
- [ ] 16.1.5 - High-dimensional data
- [ ] 16.2 - Probabilistic Latent Variables
- [ ] 16.2.1 - Generative model
- [ ] 16.2.2 - Likelihood function
- [ ] 16.2.3 - Maximum likelihood
- [ ] 16.2.4 - Factor analysis
- [ ] 16.2.5 - Independent component analysis
- [ ] 16.2.6 - Kalman filters
- [ ] 16.3 - Evidence Lower Bound
- [ ] 16.3.1 - Expectation maximization
- [ ] 16.3.2 - EM for PCA
- [ ] 16.3.3 - EM for factor analysis
- [ ] 16.4 - Nonlinear Latent Variable Models
- [ ] 16.4.1 - Nonlinear manifolds
- [ ] 16.4.2 - Likelihood function
- [ ] 16.4.3 - Discrete data
- [ ] 16.4.4 - Four approaches to generative modelling
- [ ] 17 - Generative Adversarial Networks
- [ ] 17.1 - Adversarial Training
- [ ] 17.1.1 - Loss function
- [ ] 17.1.2 - GAN training in practice
- [ ] 17.2 - Image GANs
- [ ] 17.2.1 - CycleGAN
- [ ] 18 - Normalizing Flows
- [ ] 18.1 - Coupling Flows
- [ ] 18.2 - Autoregressive Flows
- [ ] 18.3 - Continuous Flows
- [ ] 18.3.1 - Neural differential equations
- [ ] 18.3.2 - Neural ODE backpropagation
- [ ] 18.3.3 - Neural ODE flows
- [ ] 19 - Autoencoders
- [ ] 19.1 - Deterministic Autoencoders
- [ ] 19.1.1 - Linear autoencoders
- [ ] 19.1.2 - Deep autoencoders
- [ ] 19.1.3 - Sparse autoencoders
- [ ] 19.1.4 - Denoising autoencoders
- [ ] 19.1.5 - Masked autoencoders
- [ ] 19.2 - Variational Autoencoders
- [ ] 19.2.1 - Amortized inference
- [ ] 19.2.2 - The reparameterization trick
- [ ] 20 - Diffusion Models
- [ ] 20.1 - Forward Encoder
- [ ] 20.1.1 - Diffusion kernel
- [ ] 20.1.2 - Conditional distribution
- [ ] 20.2 - Reverse Decoder
- [ ] 20.2.1 - Training the decoder
- [ ] 20.2.2 - Evidence lower bound
- [ ] 20.2.3 - Rewriting the ELBO
- [ ] 20.2.4 - Predicting the noise
- [ ] 20.2.5 - Generating new samples
- [ ] 20.3 - Score Matching
- [ ] 20.3.1 - Score loss function
- [ ] 20.3.2 - Modified score loss
- [ ] 20.3.3 - Noise variance
- [ ] 20.3.4 - Stochastic differential equations
- [ ] 20.4 - Guided Diffusion
- [ ] 20.4.1 - Classifier guidance


## CS

- [ ] ADM - 2.5 (Reasoning about Efficiency)
- [ ] ADM - 2.6 (Summations)
- [ ] ADM - 2.7 (Logarithms + Applications)
- [ ] ADM - 2.8 (Properties of Logarithms)
- [ ] ADM - 2.9 (War Story!)
- [ ] ADM - 2.10 (Advanced Analysis)
- [ ] ADM - 2.11 (Exercises)
- [ ] Evaluate Reverse Polish Notation
- [ ] Generate Parentheses
- [ ] Daily Temperatures
- [ ] Car Fleet
- [ ] Largest Rectangle in Histogram
- [ ] [1.1.4 Compound Procedures](https://sarabander.github.io/sicp/html/1_002e1.xhtml#g_t1_002e1_002e4)
- [ ] [1.1.5 The Substitution Model for Procedure Application](https://sarabander.github.io/sicp/html/1_002e1.xhtml#g_t1_002e1_002e5)
- [ ] [1.1.6 Conditional Expressions and Predicates](https://sarabander.github.io/sicp/html/1_002e1.xhtml#g_t1_002e1_002e6)
- [ ] [1.1.7 Example: Square Roots by Newton’s Method](https://sarabander.github.io/sicp/html/1_002e1.xhtml#g_t1_002e1_002e7)
- [ ] [1.1.8 Procedures as Black-Box Abstractions](https://sarabander.github.io/sicp/html/1_002e1.xhtml#g_t1_002e1_002e8)
- [ ] [1.2.1 Linear Recursion and Iteration](https://sarabander.github.io/sicp/html/1_002e2.xhtml#g_t1_002e2_002e1)
- [ ] [1.2.2 Tree Recursion](https://sarabander.github.io/sicp/html/1_002e2.xhtml#g_t1_002e2_002e2)
- [ ] [1.2.3 Orders of Growth](https://sarabander.github.io/sicp/html/1_002e2.xhtml#g_t1_002e2_002e3)
- [ ] [1.2.4 Exponentiation](https://sarabander.github.io/sicp/html/1_002e2.xhtml#g_t1_002e2_002e4)
- [ ] [1.2.5 Greatest Common Divisors](https://sarabander.github.io/sicp/html/1_002e2.xhtml#g_t1_002e2_002e5)
- [ ] [1.2.6 Example: Testing for Primality](https://sarabander.github.io/sicp/html/1_002e2.xhtml#g_t1_002e2_002e6)
- [ ] [1.3.1 Procedures as Arguments](https://sarabander.github.io/sicp/html/1_002e3.xhtml#g_t1_002e3_002e1)
- [ ] [1.3.2 Constructing Procedures Using `Lambda`](https://sarabander.github.io/sicp/html/1_002e3.xhtml#g_t1_002e3_002e2)
- [ ] [1.3.3 Procedures as General Methods](https://sarabander.github.io/sicp/html/1_002e3.xhtml#g_t1_002e3_002e3)
- [ ] [1.3.4 Procedures as Returned Values](https://sarabander.github.io/sicp/html/1_002e3.xhtml#g_t1_002e3_002e4)


## Robotics

- [ ] 2.7 - Visualization Demo
- [ ] 3.1 - Basics of Lie Group and Lie Algebra
- [ ] 3.2 - Exponential and Logarithmic Mapping
- [ ] 3.3 -  Lie Algebra Derivation and Perturbation Model
- [ ] 3.4 - Practice: Sophus
- [ ] 3.5 - Similar Transform Group and Its Lie Algebra
- [ ] 3.6 - Summary
- [ ] Control Bootcamp: Overview
- [ ] Control Bootcamp: Linear Systems
- [ ] Control Bootcamp: Stability and Eigenvalues
- [ ] Control Bootcamp: Linearizing Around a Fixed Point
- [ ] Control Bootcamp: Controllability
- [ ] Control Bootcamp: Controllability, Reachability, Eigenvalue Placement
- [ ] Control Bootcamp: Controllability and the Discrete-Time Impulse Response
- [ ] Control Bootcamp: Degrees of Controllability and Gramians
- [ ] Control Bootcamp: Controllability and the PBH Test
- [ ] Control Bootcamp: Cayley-Hamilton Theorem


## Thermodynamics

- [ ] Thermo 1.1 - Semantics
- [ ] Thermo 1.2 - History
- [ ] Thermo 1.3 - Philosophy
- [ ] Thermo 1.4 - Practical applications
- [ ] Thermo 1.5 - Style
- [ ] Thermo 1.6 - Thermodynamic system and control volume
- [ ] Thermo 1.7 - Macro vs micro
- [ ] Thermo 1.8 - Properties and state of substance
- [ ] Thermo 1.9 - Processes and cycles
- [ ] Thermo 1.10 - Fundamental variables and units
- [ ] Thermo 1.11 - Zeroth law
- [ ] Thermo 1.12 - Secondary variables and units
- [ ] Thermo 2.1 - The pure substance
- [ ] Thermo 2.2 - Vapor-liquid-solid phase equilibrium
- [ ] Thermo 2.3 - Independent properties
- [ ] Thermo 2.4.1 - Ideal gas law
- [ ] Thermo 2.4.2 - Non-ideal thermal equations of state
- [ ] Thermo 2.4.3 - Compressibility factor
- [ ] Thermo 2.4.4 - Tabular thermal equations of state
- [ ] Thermo 3.1 - Mathematical preliminaries
- [ ] Thermo 3.2 - Work
- [ ] Thermo 3.3 - heat
- [ ] Therm 3.4 - Representations of the first law


***

## Archive

- [ ] W5 Exercises
- [ ] ADM - 2.2 (Big O)
- [ ] Permutation Is String
- [ ] [1.1.1 Expressions](https://sarabander.github.io/sicp/html/1_002e1.xhtml#g_t1_002e1_002e1)
- [ ] W5 Lab
- [ ] [1.1.2 Naming and the Environment](https://sarabander.github.io/sicp/html/1_002e1.xhtml#g_t1_002e1_002e2)
- [ ] Minimum Window Substring
- [ ] ADM - 2.3 (Growth and Dominance Relations)
- [ ] Review slambook 2.1
- [ ] 2.2 - Practice: Use Eigen
- [ ] 2.3 - Rotation Vectors and Euler Angles
- [ ] Sliding Window Maximum
- [ ] Valid Parentheses
- [ ] ADM - 2.4 (Working with Big O)
- [ ] [1.1.3 Evaluating Combinations](https://sarabander.github.io/sicp/html/1_002e1.xhtml#g_t1_002e1_002e3)
- [ ] 2.4 - Quaternions
- [ ] Min Stack
- [ ] W5 Homework
- [ ] W6 Neural Networks Intro
- [ ] W6 Basic Element
- [ ] W6 Networks
- [ ] W6 Choices of Activation Function
- [ ] W6 Error Backpropagation
- [ ] W6 Training
- [ ] W6 Loss Functions and Activation Functions
- [ ] W6 Exercises
- [ ] W6 Lab
- [ ] W6 Homework
- [ ] W7 Batch Gradient Descent
- [ ] W7 Running Averages
- [ ] W7 Adaptive Step Size
- [ ] W7 - Momentum
- [ ] W7 - AdaGrad and AdaDelta
- [ ] W7 - Adam
- [ ] W7 - Regularization by Weight Decay
- [ ] W7 - Regularization by Early Stopping and Dropout
- [ ] 2.5 - Affine and Projective Transformation
- [ ] W7 - Regularization by Batch Normalization
- [ ] W7 Exercises
- [ ] W7 Lab
- [ ] W7 Homework
- [ ] W8 - Introduction to CNNs
- [ ] W8 - Filters
- [ ] W8 - Max Pooling
- [ ] W8 - Typical Architecture
- [ ] W8 Exercises
- [ ] W8 Lab
- [ ] W8 Homework
- [ ] W9 - Intro to sequential models
- [ ] W9 - State Machines
- [ ] W9 - Markov Decision Processes
- [ ] W9 Exercises
- [ ] W9 Lab
- [ ] W9 Homework
- [ ] 2.6 - Practice: Eigen Geometry Module
- [ ] 1.1 - The Impact of Deep Learning
- [ ] 1.1.1 - Medical diagnosis
- [ ] 1.1.2 - Protein structure
- [ ] 1.1.3 - Image synthesis
- [ ] 1.1.4 - Large language models
- [ ] micrograd

%% kanban:settings
```
{"kanban-plugin":"basic"}
```
%%